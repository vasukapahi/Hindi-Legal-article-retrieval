{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4ca830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3518/3518 [00:03<00:00, 1147.28 examples/s]\n",
      "Map: 100%|██████████| 440/440 [00:00<00:00, 1444.81 examples/s]\n",
      "Map: 100%|██████████| 440/440 [00:00<00:00, 1481.02 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1760' max='1760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1760/1760 11:30, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.968182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.161152</td>\n",
       "      <td>0.965909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.124461</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.161923</td>\n",
       "      <td>0.972727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_accuracy so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Training Accuracy:   0.9849\n",
      "  Validation Accuracy: 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_accuracy so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Training Accuracy:   0.9847\n",
      "  Validation Accuracy: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_accuracy so early stopping is disabled\n",
      "Checkpoint destination directory ./mbert-finetuned-hindi/checkpoint-1320 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Training Accuracy:   0.9986\n",
      "  Validation Accuracy: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_accuracy so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Training Accuracy:   0.9991\n",
      "  Validation Accuracy: 0.9727\n",
      "\n",
      "Final Validation Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9727\n",
      "\n",
      "Final Test Metrics:\n",
      "Test Accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# --------------------- DATA PREPARATION ---------------------\n",
    "df = pd.read_csv(\"labelled_Hindi_Articles_2200.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(subset=[\"Heading\", \"Article\"]).rename(columns={\"Heading\": \"question\", \"Article\": \"context\"})\n",
    "df[\"label\"] = 1\n",
    "\n",
    "neg_df = df.copy()\n",
    "neg_df[\"context\"] = np.random.permutation(neg_df[\"context\"].values)\n",
    "neg_df[\"label\"] = 0\n",
    "\n",
    "combined_df = pd.concat([df, neg_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train (80%), validation (10%), test (10%)\n",
    "train_df, temp_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df[\"label\"], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --------------------- TOKENIZATION ---------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "# --------------------- MODEL SETUP ---------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# --------------------- TRAINING SETUP ---------------------\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbert-finetuned-hindi\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,  # You can increase this if not overfitting\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=3e-5,  # Lower learning rate for better stability\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate scheduler\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True  # Mixed precision for faster training (if supported by your GPU)\n",
    ")\n",
    "\n",
    "# --------------------- CALLBACK FOR ACCURACY PRINTING ---------------------\n",
    "class TrainValEvalCallback(TrainerCallback):\n",
    "    def __init__(self, train_dataset, val_dataset):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.trainer = None\n",
    "\n",
    "    def set_trainer(self, trainer):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.trainer is not None:\n",
    "            train_metrics = self.trainer.evaluate(eval_dataset=self.train_dataset, metric_key_prefix=\"train\")\n",
    "            val_metrics = self.trainer.evaluate(eval_dataset=self.val_dataset, metric_key_prefix=\"eval\")\n",
    "            print(f\"\\nEpoch {int(state.epoch)}:\")\n",
    "            print(f\"  Training Accuracy:   {train_metrics['train_accuracy']:.4f}\")\n",
    "            print(f\"  Validation Accuracy: {val_metrics['eval_accuracy']:.4f}\")\n",
    "        return control\n",
    "\n",
    "train_val_callback = TrainValEvalCallback(train_dataset, val_dataset)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[train_val_callback, EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "train_val_callback.set_trainer(trainer)\n",
    "\n",
    "# --------------------- TRAINING ---------------------\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# --------------------- FINAL EVALUATION ---------------------\n",
    "print(\"\\nFinal Validation Metrics:\")\n",
    "val_metrics = trainer.evaluate(val_dataset)\n",
    "print(f\"Validation Accuracy: {val_metrics['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "test_metrics = trainer.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
    "\n",
    "# --------------------- ARTICLE RETRIEVAL FUNCTION ---------------------\n",
    "def find_relevant_article(question: str, top_k=3):\n",
    "    \"\"\"Retrieve top-k relevant articles for a question\"\"\"\n",
    "    original_contexts = df[\"context\"].unique().tolist()\n",
    "    scores = []\n",
    "    for context in original_contexts:\n",
    "        inputs = tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        match_prob = torch.softmax(logits, dim=1)[0][1].item()\n",
    "        scores.append((context, match_prob))\n",
    "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "# Example usage:\n",
    "# results = find_relevant_article(\"Your question here\")\n",
    "# for idx, (context, score) in enumerate(results, 1):\n",
    "#     print(f\"Match {idx} (Score: {score:.4f}):\\n{context}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45cab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Match (Confidence: 100.0%):\n",
      "राजस्थान आबकारी अधिनियम, 1950 (Rajasthan Excise Act, 1950) के तहत धारा 45 एक महत्वपूर्ण प्रावधान है, जो अधिकारियों और सरकार द्वारा अधिकृत व्यक्तियों को अपराधियों को गिरफ्तार करने, सामान जब्त करने और संदिग्ध व्यक्तियों या वस्तुओं को निरोध (Detention) में रखने का अधिकार देती है। यह प्रावधान आबकारी राजस्व (Excise Revenue) की सुरक्षा सुनिश्चित करता है और अवैध गतिविधियों को रोकने में सहायक है। इस लेख में, हम धारा 45 के सभी प्रावधानों का सरल और विस्तृत तरीके से विश्लेषण करेंगे, साथ ही इसे अधिनियम की अ...\n",
      "\n",
      "Top 2 Match (Confidence: 100.0%):\n",
      "भारतीय नागरिक सुरक्षा संहिता 2023, जो 1 जुलाई, 2024 को लागू हुई, ने दंड प्रक्रिया संहिता की जगह ले ली है। यह लेख नई संहिता की धारा 45 से 50 में उल्लिखित गिरफ्तारी से संबंधित प्रक्रियाओं की व्याख्या करता है। गिरफ्तार किए जाने वाले व्यक्तियों का पीछा करना (धारा 45) धारा 45 के तहत, एक पुलिस अधिकारी के पास भारत के किसी भी स्थान पर, बिना वारंट के भी, किसी भी व्यक्ति का पीछा करने का अधिकार है, जिसे वे गिरफ्तार करने के लिए अधिकृत हैं। यह प्रावधान सुनिश्चित करता है कि कानून प्रवर्तन उन संदिग्धों का पीछा...\n",
      "\n",
      "Top 3 Match (Confidence: 100.0%):\n",
      "भारतीय न्याय संहिता 2023, जो 1 जुलाई, 2024 को लागू हुई, ने भारतीय दंड संहिता की जगह ले ली है। यह लेख नई संहिता के अध्याय IV में उल्लिखित उकसाने, आपराधिक षडयंत्र और प्रयास से संबंधित प्रावधानों पर केंद्रित है। दुष्प्रेरण की परिभाषा (धारा 45) (Definition of Abetment) किसी व्यक्ति को किसी कार्य को करने के लिए उकसाना कहा जाता है यदि वह: 1. किसी को उस कार्य को करने के लिए उकसाता है। 2. उस कार्य को करने के लिए दूसरों के साथ षडयंत्र में शामिल होता है, जिसके परिणामस्वरूप कोई कार्य या अवैध चूक होती है। 3...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_results = find_relevant_article(\"राजस्थान आबकारी अधिनियम, 1950 की धारा 45 के तहत गिरफ्तारी, जप्ती और निरोध की श?\")\n",
    "for i, (article, score) in enumerate(search_results, 1):\n",
    "    print(f\"Top {i} Match (Confidence: {score*100:.1f}%):\")\n",
    "    print(article[:500] + \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cdaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c37d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c868493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41272f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc698f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad951c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6641cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
